# papers
# Background research for Bezoku
## Challenges and Strategies in Cross-Cultural NLP, Hershcovich et al https://aclanthology.org/2022.acl-long.482.pdf
## On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?, Bender et al https://dl.acm.org/doi/pdf/10.1145/3442188.3445922
## The Importance of Modeling Social Factors of Language:Theory and Practice, Hovy et al https://aclanthology.org/2021.naacl-main.49.pdf
## Comparing Explanation Faithfulness between Multilingual and Monolingual Fine-tuned Language Models, Zhao et al https://aclanthology.org/2024.naacl-long.178.pdf
## The State and Fate of Linguistic Diversity and Inclusion in the NLP World, Joshi et al https://aclanthology.org/2020.acl-main.560.pdf
## The Zeno’s Paradox of ‘Low-Resource’ Languages, Nigatu et al https://aclanthology.org/2024.emnlp-main.983.pdf
## Knowledge of cultural moral norms in large language models, Ramezani et al https://aclanthology.org/2023.acl-long.26.pdf
## Language Model Tokenizers Introduce Unfairness Between Languages, Petrov et al https://arxiv.org/pdf/2305.15425
# Guide to underlying formats and technologies
## Universal Dependencies CoNLL-U format https://universaldependencies.org/format.html
## Extended Long Short Term Memory, Hochreiter et al https://arxiv.org/pdf/2405.04517
## A Tensor Compiler with Automatic Data Packing for Simple and Efficient Fully Homomorphic Encryption, Krastev et all https://dl.acm.org/doi/pdf/10.1145/3656382
## Automatic Annotation of Enhanced Universal Dependencies for Brazilian Portuguese, de Souza et al https://sol.sbc.org.br/index.php/stil/article/view/31134/30937
# Related papers
## Rule-based semantic interpretation for Universal Dependencies, Findlay et al https://aclanthology.org/2023.udw-1.6.pdf
## AMR Parsing is Far from Solved: GrAPES, the Granular AMR Parsing Evaluation Suite, Groschwitz et al https://arxiv.org/pdf/2312.03480
## Neural Semantic Parsing with Extremely Rich Symbolic Meaning Representations, Zhang et al https://research.rug.nl/en/publications/neural-semantic-parsing-with-extremely-rich-symbolic-meaning-repr
## T-FREE: Tokenizer-Free Generative LLMs via Sparse Representations for Memory-Efficient Embeddings, Deiseroth et al https://arxiv.org/html/2406.19223v1
## The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits, Ma et al https://arxiv.org/pdf/2402.17764
## DATA ANNEALING TRANSFER LEARNING PROCEDURE FOR INFORMAL LANGUAGE UNDERSTANDING TASKS, Anonymous authors ICLR https://openreview.net/attachment?id=HJlys1BtwB&name=original_pdf
## Representing Low-Resource Languages and Dialects: Improved Neural Methods for Spoken Language Processing, Martijn Bartelds https://research.rug.nl/en/publications/representing-low-resource-languages-and-dialects-improved-neural-
## Adapting Monolingual Models: Data can be Scarce when Language Similarity is High, de Vries et al https://research.rug.nl/en/publications/adapting-monolingual-models-data-can-be-scarce-when-language-simi

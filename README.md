# papers
# Main academic papers and research
# Background research for Bezoku
## Challenges and Strategies in Cross-Cultural NLP, Hershcovich et al https://aclanthology.org/2022.acl-long.482.pdf
## On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?, Bender et al https://dl.acm.org/doi/pdf/10.1145/3442188.3445922
# Guide to underlying formats and technologies
## Universal Dependencies CoNLL-U format https://universaldependencies.org/format.html
## Extended Long Short Term Memory, Hochreiter et al https://arxiv.org/pdf/2405.04517
## A Tensor Compiler with Automatic Data Packing for Simple and Efficient Fully Homomorphic Encryption, Krastev et all https://dl.acm.org/doi/pdf/10.1145/3656382
# Related papers
## Rule-based semantic interpretation for Universal Dependencies, Findlay et al https://aclanthology.org/2023.udw-1.6.pdf
## AMR Parsing is Far from Solved: GrAPES, the Granular AMR Parsing Evaluation Suite, Groschwitz et al https://arxiv.org/pdf/2312.03480
## T-FREE: Tokenizer-Free Generative LLMs via Sparse Representations for Memory-Efficient Embeddings, Deiseroth et al https://arxiv.org/html/2406.19223v1
## The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits, Ma et al https://arxiv.org/pdf/2402.17764
